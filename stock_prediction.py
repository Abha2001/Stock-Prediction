# -*- coding: utf-8 -*-
"""Stock Prediction using LSTM.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1rGk6sTAITsVH8AxMi1S-dmmoMS31mXeF

# Import Libraries
"""

import pandas_datareader.data as web
import pandas as pd

import zipfile

from datetime import datetime

import matplotlib.pyplot as plt

import numpy as np

import tensorflow as tf

from sklearn.preprocessing import MinMaxScaler
from sklearn.metrics import mean_squared_error
from sklearn.model_selection import train_test_split

from keras.models import Sequential, Model
from keras.layers import Dense, LSTM, LayerNormalization, Dropout, Input, MultiHeadAttention, Add, GlobalAveragePooling1D
import math

"""# Collect Data"""

df = web.DataReader("AAPL", "av-daily", start=datetime(1990, 2, 9),end=datetime(2024, 5, 24),api_key='4WGCBLTQI4LZBZKW')

df.head()

df2 = df.reset_index()['close']

df2.shape

plt.plot(df2)
plt.show()

scaler = MinMaxScaler(feature_range=(0,1))
df3 = scaler.fit_transform(np.array(df2).reshape(-1,1))

df3.shape

"""# Preprocess Data"""

training_size = int(len(df3)*0.65)
test_size = len(df3) - training_size
train_data, test_data = df3[0:training_size,:], df3[training_size:len(df3),:1]

def create_dataset(dataset, time_step=1):
  dataX, dataY = [], []
  for i in range(len(dataset)-time_step-1):
    dataX.append(dataset[i:(i+time_step), 0])
    dataY.append(dataset[i + time_step, 0])
  return np.array(dataX), np.array(dataY)

time_step = 100
x_train, y_train = create_dataset(train_data, time_step)
x_test, y_test = create_dataset(test_data, time_step)

x_train = x_train.reshape(x_train.shape[0], x_train.shape[1], 1)
x_test = x_test.reshape(x_test.shape[0], x_test.shape[1],1)

def transformer_encoder(inputs, head_size, num_heads, ff_dim, dropout=0):
    # Attention and Normalization
    #x = LayerNormalization(epsilon=1e-6)(inputs)
    x = MultiHeadAttention(key_dim=head_size, num_heads=num_heads, dropout=dropout)(inputs, inputs)
    x = Dense(ff_dim, activation="relu")(x)
    x = Dropout(dropout)(x)
    x = Add()([x, inputs])

    # Feed Forward Part
    y = LayerNormalization(epsilon=1e-6)(x)
    y = Dense(ff_dim, activation="relu")(y)
    y = Dropout(dropout)(y)
    y = Dense(inputs.shape[-1])(y)
    return Add()([y, x])

def build_transformer_model(input_shape, head_size, num_heads, ff_dim, num_layers, dropout=0):
    inputs = Input(shape=input_shape)
    x = inputs

    # Create multiple layers of the Transformer block
    for _ in range(num_layers):
        x = transformer_encoder(x, head_size, num_heads, ff_dim, dropout)

    # Final part of the model
    x = GlobalAveragePooling1D()(x)
    #x = LayerNormalization(epsilon=1e-6)(x)
    outputs = Dense(1, activation="linear")(x)

    # Compile model
    model = Model(inputs=inputs, outputs=outputs)
    return model

# Model parameters
input_shape = x_train.shape[1:]
head_size = 64
num_heads = 8
ff_dim = 32
num_layers = 1
dropout = 0.10

# Build the model
transformer_model = build_transformer_model(input_shape, head_size, num_heads, ff_dim, num_layers, dropout)
transformer_model.summary()

#LSTM

# model = Sequential()
# model.add(LSTM(512, return_sequences=True, input_shape=(x_train.shape[1],1)))
# model.add(LayerNormalization())
# model.add(Dropout(0.2))
# model.add(LSTM(512, return_sequences=True))
# model.add(LayerNormalization())
# model.add(Dropout(0.2))
# model.add(LSTM(512))
# model.add(LayerNormalization())
# model.add(Dense(1))
# model.add(Dropout(0.2))
# model.compile(loss='mean_squared_error', optimizer='adam')

# Compile the model
optimizer = tf.keras.optimizers.Adam()
transformer_model.compile(optimizer=optimizer, loss='mean_squared_error')

transformer_model.fit(x_train, y_train, validation_data=(x_test, y_test), epochs=100, batch_size=64, verbose=1)

train_predict = transformer_model.predict(x_train)
test_predict = transformer_model.predict(x_test)

train_predict = scaler.inverse_transform(train_predict)
test_predict = scaler.inverse_transform(test_predict)

math.sqrt(mean_squared_error(y_train, train_predict))

math.sqrt(mean_squared_error(y_test, test_predict))

look_back = 100
trainPredictPlot = np.empty_like(df3)
trainPredictPlot[:, :] = np.nan
trainPredictPlot[look_back:len(train_predict)+look_back, :] = train_predict
testPredictPlot = np.empty_like(df3)
testPredictPlot[:, :] = np.nan
testPredictPlot[len(train_predict)+(look_back*2)+1:len(df3)-1, :] = test_predict
plt.plot(scaler.inverse_transform(df3))
plt.plot(trainPredictPlot)
plt.plot(testPredictPlot)
plt.show()

x_input = test_data[len(test_data)-100:].reshape(1,-1)
x_input.shape

temp_input = list(x_input)
temp_input = temp_input[0].tolist()

lst_output = []
n_steps=100
i=0
while(i<30):
  if len(temp_input)>100:
    x_input = np.array(temp_input[1:])
    print("{} day input {}".format(i,x_input))
    x_input=x_input.reshape(1,-1)
    x_input = x_input.reshape((1, n_steps, 1))
    yhat = transformer_model.predict(x_input, verbose=0)
    print("{} day output {}".format(i,yhat))
    temp_input.extend(yhat[0].tolist())
    temp_input = temp_input[1:]
    lst_output.extend(yhat.tolist())
    i=i+1
  else:
    x_input = x_input.reshape((1, n_steps,1))
    yhat = transformer_model.predict(x_input, verbose=0)
    print(yhat[0])
    temp_input.extend(yhat[0].tolist())
    print(len(temp_input))
    lst_output.extend(yhat.tolist())
    i=i+1
print(lst_output)

day_new = np.arange(1,101)
day_pred = np.arange(101,131)

len(df3)

plt.plot(day_new, scaler.inverse_transform(df3[len(df3)-100:]))
plt.plot(day_pred, scaler.inverse_transform(lst_output))

df4 = df3.tolist()
df4.extend(lst_output)
plt.plot(df4[len(df3)-1000:])